{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33e92cd0550dcfd1",
   "metadata": {},
   "source": [
    "# Ejercicio 5: Modelo Probabilístico\n",
    "\n",
    "## Objetivo de la práctica\n",
    "- Aplicar paso a paso técnicas de preprocesamiento, evaluando el impacto de cada etapa en el número de tokens y en el vocabulario final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88982921c8872f8f",
   "metadata": {},
   "source": [
    "## Parte 0: Carga del Corpus"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-28T17:48:46.441510Z",
     "start_time": "2025-05-28T17:48:45.519415Z"
    }
   },
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "newsgroupsdocs = newsgroups.data"
   ],
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRecursionError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdatasets\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m fetch_20newsgroups\n\u001B[32m      3\u001B[39m newsgroups = fetch_20newsgroups(subset=\u001B[33m'\u001B[39m\u001B[33mall\u001B[39m\u001B[33m'\u001B[39m, remove=(\u001B[33m'\u001B[39m\u001B[33mheaders\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mfooters\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mquotes\u001B[39m\u001B[33m'\u001B[39m))\n\u001B[32m      4\u001B[39m newsgroupsdocs = newsgroups.data\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/__init__.py:73\u001B[39m\n\u001B[32m     62\u001B[39m \u001B[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001B[39;00m\n\u001B[32m     63\u001B[39m \u001B[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001B[39;00m\n\u001B[32m     64\u001B[39m \u001B[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     67\u001B[39m \u001B[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001B[39;00m\n\u001B[32m     68\u001B[39m \u001B[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001B[39;00m\n\u001B[32m     69\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (  \u001B[38;5;66;03m# noqa: F401 E402\u001B[39;00m\n\u001B[32m     70\u001B[39m     __check_build,\n\u001B[32m     71\u001B[39m     _distributor_init,\n\u001B[32m     72\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m73\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mbase\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m clone  \u001B[38;5;66;03m# noqa: E402\u001B[39;00m\n\u001B[32m     74\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutils\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_show_versions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m show_versions  \u001B[38;5;66;03m# noqa: E402\u001B[39;00m\n\u001B[32m     76\u001B[39m _submodules = [\n\u001B[32m     77\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mcalibration\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     78\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mcluster\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    114\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mcompose\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    115\u001B[39m ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/base.py:14\u001B[39m\n\u001B[32m     11\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mwarnings\u001B[39;00m\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mcollections\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m defaultdict\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m __version__\n\u001B[32m     17\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_config\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m config_context, get_config\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/numpy/__init__.py:472\u001B[39m\n\u001B[32m    469\u001B[39m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[32m    471\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m sys.platform == \u001B[33m\"\u001B[39m\u001B[33mdarwin\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m472\u001B[39m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01m.\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m exceptions\n\u001B[32m    473\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m warnings.catch_warnings(record=\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m w:\n\u001B[32m    474\u001B[39m         _mac_os_check()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:1412\u001B[39m, in \u001B[36m_handle_fromlist\u001B[39m\u001B[34m(module, fromlist, import_, recursive)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/numpy/__init__.py:352\u001B[39m, in \u001B[36m__getattr__\u001B[39m\u001B[34m(attr)\u001B[39m\n\u001B[32m    350\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m ctypeslib\n\u001B[32m    351\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m attr == \u001B[33m\"\u001B[39m\u001B[33mexceptions\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m352\u001B[39m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mexceptions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mexceptions\u001B[39;00m\n\u001B[32m    353\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m exceptions\n\u001B[32m    354\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m attr == \u001B[33m\"\u001B[39m\u001B[33mtesting\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/numpy/__init__.py:352\u001B[39m, in \u001B[36m__getattr__\u001B[39m\u001B[34m(attr)\u001B[39m\n\u001B[32m    350\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m ctypeslib\n\u001B[32m    351\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m attr == \u001B[33m\"\u001B[39m\u001B[33mexceptions\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m352\u001B[39m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mexceptions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mexceptions\u001B[39;00m\n\u001B[32m    353\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m exceptions\n\u001B[32m    354\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m attr == \u001B[33m\"\u001B[39m\u001B[33mtesting\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "    \u001B[31m[... skipping similar frames: __getattr__ at line 352 (2948 times)]\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/numpy/__init__.py:352\u001B[39m, in \u001B[36m__getattr__\u001B[39m\u001B[34m(attr)\u001B[39m\n\u001B[32m    350\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m ctypeslib\n\u001B[32m    351\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m attr == \u001B[33m\"\u001B[39m\u001B[33mexceptions\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m352\u001B[39m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mexceptions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mexceptions\u001B[39;00m\n\u001B[32m    353\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m exceptions\n\u001B[32m    354\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m attr == \u001B[33m\"\u001B[39m\u001B[33mtesting\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:471\u001B[39m, in \u001B[36m_lock_unlock_module\u001B[39m\u001B[34m(name)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:311\u001B[39m, in \u001B[36macquire\u001B[39m\u001B[34m(self)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:170\u001B[39m, in \u001B[36m__enter__\u001B[39m\u001B[34m(self)\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen importlib._bootstrap>:132\u001B[39m, in \u001B[36msetdefault\u001B[39m\u001B[34m(self, key, default)\u001B[39m\n",
      "\u001B[31mRecursionError\u001B[39m: maximum recursion depth exceeded"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "c6196ea9cb414396",
   "metadata": {},
   "source": [
    "## Parte 1: Tokenización\n",
    "\n",
    "### Actividad \n",
    "1. Tokeniza los documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfb0f2438c9c0144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenización completada exitosamente\n",
      "Número de documentos tokenizados: 18846\n",
      "Documento 1: ['I', 'am', 'sure', 'some', 'bashers', 'of', 'Pens', 'fans', 'are', 'pretty', 'confused', 'about', 'the', 'lack', 'of', 'any', 'kind', 'of', 'posts', 'about']...\n",
      "Documento 2: ['My', 'brother', 'is', 'in', 'the', 'market', 'for', 'a', 'high-performance', 'video', 'card', 'that', 'supports', 'VESA', 'local', 'bus', 'with', '1-2MB', 'RAM', '.']...\n",
      "Documento 3: ['Finally', 'you', 'said', 'what', 'you', 'dream', 'about', '.', 'Mediterranean', '?', '?', '?', '?', 'That', 'was', 'new', '....', 'The', 'area', 'will']...\n",
      "Documento 4: ['Think', '!', 'It', \"'s\", 'the', 'SCSI', 'card', 'doing', 'the', 'DMA', 'transfers', 'NOT', 'the', 'disks', '...', 'The', 'SCSI', 'card', 'can', 'do']...\n",
      "Documento 5: ['1', ')', 'I', 'have', 'an', 'old', 'Jasmine', 'drive', 'which', 'I', 'can', 'not', 'use', 'with', 'my', 'new', 'system', '.', 'My', 'understanding']...\n",
      "Número total de documentos: 18846\n",
      "Número total de palabras en el corpus: 4769227\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_corpus(corpus):\n",
    "    return [word_tokenize(doc) for doc in corpus]\n",
    "\n",
    "tokenized_docs = tokenize_corpus(newsgroupsdocs)\n",
    "\n",
    "# Guardar el corpus tokenizado en un archivo\n",
    "import pickle\n",
    "with open('tokenized_docs.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenized_docs, f)\n",
    "\n",
    "# Cargar el corpus tokenizado desde el archivo\n",
    "with open('tokenized_docs.pkl', 'rb') as f:\n",
    "    loaded_tokenized_docs = pickle.load(f)\n",
    "\n",
    "# Verificar que el corpus cargado es igual al original\n",
    "assert loaded_tokenized_docs == tokenized_docs, \"Los corpus no coinciden\"\n",
    "print(\"Tokenización completada exitosamente\")\n",
    "print(f\"Número de documentos tokenizados: {len(tokenized_docs)}\")\n",
    "\n",
    "# Mostrar algunos ejemplos de documentos tokenizados\n",
    "for i in range(5):\n",
    "    print(f\"Documento {i+1}: {loaded_tokenized_docs[i][:20]}...\")  # Mostrar solo las primeras 20 palabras\n",
    "# Mostrar el número total de documentos\n",
    "print(f\"Número total de documentos: {len(loaded_tokenized_docs)}\")\n",
    "# Mostrar el número total de palabras en el corpus\n",
    "total_words = sum(len(doc) for doc in loaded_tokenized_docs)\n",
    "print(f\"Número total de palabras en el corpus: {total_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ecfc1e6638e9d2",
   "metadata": {},
   "source": [
    "## Parte 2: Normalización\n",
    "\n",
    "### Actividad \n",
    "1. Convierte todos los tokens a minúsculas.\n",
    "2. Elimina puntuación y símbolos no alfabéticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc67a424c6550fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesamiento completado exitosamente\n",
      "Número de documentos procesados: 18846\n",
      "Documento 1: ['i', 'am', 'sure', 'some', 'bashers', 'of', 'pens', 'fans', 'are', 'pretty', 'confused', 'about', 'the', 'lack', 'of', 'any', 'kind', 'of', 'posts', 'about']...\n",
      "Documento 2: ['my', 'brother', 'is', 'in', 'the', 'market', 'for', 'a', 'video', 'card', 'that', 'supports', 'vesa', 'local', 'bus', 'with', 'ram', 'does', 'anyone', 'have']...\n",
      "Documento 3: ['finally', 'you', 'said', 'what', 'you', 'dream', 'about', 'mediterranean', 'that', 'was', 'new', 'the', 'area', 'will', 'be', 'greater', 'after', 'some', 'years', 'like']...\n",
      "Documento 4: ['think', 'it', 'the', 'scsi', 'card', 'doing', 'the', 'dma', 'transfers', 'not', 'the', 'disks', 'the', 'scsi', 'card', 'can', 'do', 'dma', 'transfers', 'containing']...\n",
      "Documento 5: ['i', 'have', 'an', 'old', 'jasmine', 'drive', 'which', 'i', 'can', 'not', 'use', 'with', 'my', 'new', 'system', 'my', 'understanding', 'is', 'that', 'i']...\n",
      "Número total de documentos procesados: 18846\n"
     ]
    }
   ],
   "source": [
    "#convierte todos los tokens a minusculas, elimina puntuacion y simbolos alfanumericos con nltk\n",
    "import string\n",
    "def preprocess_tokens(tokenized_docs):\n",
    "    processed_docs = []\n",
    "    for doc in tokenized_docs:\n",
    "        # Convertir a minúsculas y eliminar puntuación\n",
    "        doc = [word.lower() for word in doc if word.isalpha()]\n",
    "        processed_docs.append(doc)\n",
    "    return processed_docs\n",
    "processed_docs = preprocess_tokens(loaded_tokenized_docs)\n",
    "# Guardar el corpus procesado en un archivo\n",
    "with open('processed_docs.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_docs, f)\n",
    "# Cargar el corpus procesado desde el archivo\n",
    "with open('processed_docs.pkl', 'rb') as f:\n",
    "    loaded_processed_docs = pickle.load(f)\n",
    "# Verificar que el corpus procesado es igual al original\n",
    "assert loaded_processed_docs == processed_docs, \"Los corpus procesados no coinciden\"\n",
    "print(\"Procesamiento completado exitosamente\")\n",
    "print(f\"Número de documentos procesados: {len(processed_docs)}\")\n",
    "# Mostrar algunos ejemplos de documentos procesados\n",
    "for i in range(5):\n",
    "    print(f\"Documento {i+1}: {loaded_processed_docs[i][:20]}...\")  # Mostrar solo las primeras 20 palabras\n",
    "# Mostrar el número total de documentos procesados\n",
    "print(f\"Número total de documentos procesados: {len(loaded_processed_docs)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973153ad553db841",
   "metadata": {},
   "source": [
    "## Parte 3: Eliminación de Stopwords\n",
    "\n",
    "### Actividad \n",
    "1. Elimina las palabras vacías usando una lista estándar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "477c7bcd5c2d0391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtrado de palabras vacías completado exitosamente\n",
      "Número de documentos filtrados: 18846\n",
      "Documento 1: ['sure', 'bashers', 'pens', 'fans', 'pretty', 'confused', 'lack', 'kind', 'posts', 'recent', 'pens', 'massacre', 'devils', 'actually', 'bit', 'puzzled', 'bit', 'relieved', 'however', 'going']...\n",
      "Documento 2: ['brother', 'market', 'video', 'card', 'supports', 'vesa', 'local', 'bus', 'ram', 'anyone', 'diamond', 'stealth', 'pro', 'local', 'bus', 'orchid', 'farenheit', 'ati', 'graphics', 'ultra']...\n",
      "Documento 3: ['finally', 'said', 'dream', 'mediterranean', 'new', 'area', 'greater', 'years', 'like', 'holocaust', 'numbers', 'july', 'usa', 'sweden', 'april', 'still', 'cold', 'changed', 'calendar', 'nothing']...\n",
      "Documento 4: ['think', 'scsi', 'card', 'dma', 'transfers', 'disks', 'scsi', 'card', 'dma', 'transfers', 'containing', 'data', 'scsi', 'devices', 'attached', 'wants', 'important', 'feature', 'scsi', 'ability']...\n",
      "Documento 5: ['old', 'jasmine', 'drive', 'use', 'new', 'system', 'understanding', 'upsate', 'driver', 'modern', 'one', 'order', 'gain', 'compatability', 'system', 'anyone', 'know', 'inexpensive', 'program', 'seen']...\n",
      "Número total de documentos filtrados: 18846\n"
     ]
    }
   ],
   "source": [
    "#elimina las palabras vacias usando una lista estandar de nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(tokenized_docs, stop_words):\n",
    "    filtered_docs = []\n",
    "    for doc in tokenized_docs:\n",
    "        filtered_doc = [word for word in doc if word not in stop_words]\n",
    "        filtered_docs.append(filtered_doc)\n",
    "    return filtered_docs\n",
    "filtered_docs = remove_stopwords(loaded_processed_docs, stop_words)\n",
    "# Guardar el corpus filtrado en un archivo\n",
    "with open('filtered_docs.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_docs, f)\n",
    "# Cargar el corpus filtrado desde el archivo\n",
    "with open('filtered_docs.pkl', 'rb') as f:\n",
    "    loaded_filtered_docs = pickle.load(f)\n",
    "# Verificar que el corpus filtrado es igual al original\n",
    "assert loaded_filtered_docs == filtered_docs, \"Los corpus filtrados no coinciden\"\n",
    "print(\"Filtrado de palabras vacías completado exitosamente\")\n",
    "print(f\"Número de documentos filtrados: {len(filtered_docs)}\")\n",
    "# Mostrar algunos ejemplos de documentos filtrados          \n",
    "for i in range(5):\n",
    "    print(f\"Documento {i+1}: {loaded_filtered_docs[i][:20]}...\")  # Mostrar solo las primeras 20 palabras\n",
    "# Mostrar el número total de documentos filtrados\n",
    "print(f\"Número total de documentos filtrados: {len(loaded_filtered_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f090bfc7868f8",
   "metadata": {},
   "source": [
    "## Parte 4: Stemming o Lematización\n",
    "\n",
    "### Actividad\n",
    "1. Aplica stemming.\n",
    "2. Aplica lematización.\n",
    "3. Compara ambas técnicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd9ff693047bd948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming y lematización completados exitosamente\n",
      "Número de documentos con stemming: 18846\n",
      "Número de documentos con lematización: 18846\n",
      "Documento 1 con stemming: ['sure', 'basher', 'pen', 'fan', 'pretti', 'confus', 'lack', 'kind', 'post', 'recent', 'pen', 'massacr', 'devil', 'actual', 'bit', 'puzzl', 'bit', 'reliev', 'howev', 'go']...\n",
      "Documento 1 con lematización: ['sure', 'bashers', 'pen', 'fan', 'pretty', 'confused', 'lack', 'kind', 'post', 'recent', 'pen', 'massacre', 'devil', 'actually', 'bit', 'puzzled', 'bit', 'relieved', 'however', 'going']...\n",
      "Documento 2 con stemming: ['brother', 'market', 'video', 'card', 'support', 'vesa', 'local', 'bu', 'ram', 'anyon', 'diamond', 'stealth', 'pro', 'local', 'bu', 'orchid', 'farenheit', 'ati', 'graphic', 'ultra']...\n",
      "Documento 2 con lematización: ['brother', 'market', 'video', 'card', 'support', 'vesa', 'local', 'bus', 'ram', 'anyone', 'diamond', 'stealth', 'pro', 'local', 'bus', 'orchid', 'farenheit', 'ati', 'graphic', 'ultra']...\n",
      "Documento 3 con stemming: ['final', 'said', 'dream', 'mediterranean', 'new', 'area', 'greater', 'year', 'like', 'holocaust', 'number', 'juli', 'usa', 'sweden', 'april', 'still', 'cold', 'chang', 'calendar', 'noth']...\n",
      "Documento 3 con lematización: ['finally', 'said', 'dream', 'mediterranean', 'new', 'area', 'greater', 'year', 'like', 'holocaust', 'number', 'july', 'usa', 'sweden', 'april', 'still', 'cold', 'changed', 'calendar', 'nothing']...\n",
      "Documento 4 con stemming: ['think', 'scsi', 'card', 'dma', 'transfer', 'disk', 'scsi', 'card', 'dma', 'transfer', 'contain', 'data', 'scsi', 'devic', 'attach', 'want', 'import', 'featur', 'scsi', 'abil']...\n",
      "Documento 4 con lematización: ['think', 'scsi', 'card', 'dma', 'transfer', 'disk', 'scsi', 'card', 'dma', 'transfer', 'containing', 'data', 'scsi', 'device', 'attached', 'want', 'important', 'feature', 'scsi', 'ability']...\n",
      "Documento 5 con stemming: ['old', 'jasmin', 'drive', 'use', 'new', 'system', 'understand', 'upsat', 'driver', 'modern', 'one', 'order', 'gain', 'compat', 'system', 'anyon', 'know', 'inexpens', 'program', 'seen']...\n",
      "Documento 5 con lematización: ['old', 'jasmine', 'drive', 'use', 'new', 'system', 'understanding', 'upsate', 'driver', 'modern', 'one', 'order', 'gain', 'compatability', 'system', 'anyone', 'know', 'inexpensive', 'program', 'seen']...\n",
      "Número total de documentos con stemming: 18846\n",
      "Número total de documentos con lematización: 18846\n",
      "\n",
      "=== COMPARACIÓN STEMMING vs LEMATIZACIÓN ===\n",
      "Original: running | Stemming: run | Lematización: running\n",
      "Original: better | Stemming: better | Lematización: better\n",
      "Original: flies | Stemming: fli | Lematización: fly\n",
      "Original: geese | Stemming: gees | Lematización: goose\n",
      "Original: children | Stemming: children | Lematización: child\n",
      "Original: studies | Stemming: studi | Lematización: study\n"
     ]
    }
   ],
   "source": [
    "# aplicar stemming, lematización y compara ambas técnicas\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "def stem_tokens(tokenized_docs):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_docs = []\n",
    "    for doc in tokenized_docs:\n",
    "        stemmed_doc = [stemmer.stem(word) for word in doc]\n",
    "        stemmed_docs.append(stemmed_doc)\n",
    "    return stemmed_docs\n",
    "\n",
    "def lemmatize_tokens(tokenized_docs):   \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_docs = []\n",
    "    for doc in tokenized_docs:\n",
    "        lemmatized_doc = [lemmatizer.lemmatize(word) for word in doc]\n",
    "        lemmatized_docs.append(lemmatized_doc)\n",
    "    return lemmatized_docs\n",
    "\n",
    "stemmed_docs = stem_tokens(loaded_filtered_docs)\n",
    "lemmatized_docs = lemmatize_tokens(loaded_filtered_docs)\n",
    "\n",
    "# Guardar los documentos con stemming y lematización en archivos\n",
    "with open('stemmed_docs.pkl', 'wb') as f:\n",
    "    pickle.dump(stemmed_docs, f)\n",
    "with open('lemmatized_docs.pkl', 'wb') as f:\n",
    "    pickle.dump(lemmatized_docs, f)\n",
    "\n",
    "# Cargar los documentos con stemming y lematización desde los archivos\n",
    "with open('stemmed_docs.pkl', 'rb') as f:\n",
    "    loaded_stemmed_docs = pickle.load(f)\n",
    "with open('lemmatized_docs.pkl', 'rb') as f:\n",
    "    loaded_lemmatized_docs = pickle.load(f)\n",
    "\n",
    "# Verificar que los documentos con stemming y lematización son iguales a los originales\n",
    "assert loaded_stemmed_docs == stemmed_docs, \"Los documentos con stemming no coinciden\"\n",
    "assert loaded_lemmatized_docs == lemmatized_docs, \"Los documentos con lematización no coinciden\"\n",
    "print(\"Stemming y lematización completados exitosamente\")\n",
    "print(f\"Número de documentos con stemming: {len(stemmed_docs)}\")\n",
    "print(f\"Número de documentos con lematización: {len(lemmatized_docs)}\")\n",
    "\n",
    "# Mostrar algunos ejemplos de documentos con stemming y lematización\n",
    "for i in range(5):\n",
    "    print(f\"Documento {i+1} con stemming: {loaded_stemmed_docs[i][:20]}...\")  # Mostrar solo las primeras 20 palabras\n",
    "    print(f\"Documento {i+1} con lematización: {loaded_lemmatized_docs[i][:20]}...\")  # Mostrar solo las primeras 20 palabras\n",
    "\n",
    "# Mostrar el número total de documentos con stemming y lematización\n",
    "print(f\"Número total de documentos con stemming: {len(loaded_stemmed_docs)}\")\n",
    "print(f\"Número total de documentos con lematización: {len(loaded_lemmatized_docs)}\")\n",
    "\n",
    "# Comparación entre stemming y lematización\n",
    "print(\"\\n=== COMPARACIÓN STEMMING vs LEMATIZACIÓN ===\")\n",
    "# Ejemplo de palabras para comparar\n",
    "sample_words = ['running', 'better', 'flies', 'geese', 'children', 'studies']\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for word in sample_words:\n",
    "    stemmed = stemmer.stem(word)\n",
    "    lemmatized = lemmatizer.lemmatize(word)\n",
    "    print(f\"Original: {word} | Stemming: {stemmed} | Lematización: {lemmatized}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
